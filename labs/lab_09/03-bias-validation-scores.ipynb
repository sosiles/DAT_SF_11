{
 "metadata": {
  "name": "",
  "signature": "sha256:ba00421377d8b91d18a89c9acb6952b45bdac51e16cbd9f88ff1364c95b10775"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook is based on tutorial give by [Jake Vanderplas](http://www.vanderplas.com) at PyCon 2014.</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bias-Variance Tradeoff"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Standard imports\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import metrics\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run bias_variance.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the above figure, we see fits for three different values of `d`.\n",
      "For `d = 1`, the data is under-fit. This means that the model is too\n",
      "simplistic: no straight line will ever be a good fit to this data. In\n",
      "this case, we say that the model suffers from high bias. The model\n",
      "itself is biased, and this will be reflected in the fact that the data\n",
      "is poorly fit. At the other extreme, for `d = 6` the data is over-fit.\n",
      "This means that the model has too many free parameters (6 in this case)\n",
      "which can be adjusted to perfectly fit the training data. If we add a\n",
      "new point to this plot, though, chances are it will be very far from\n",
      "the curve representing the degree-6 fit. In this case, we say that the\n",
      "model suffers from high variance. The reason for the term \"high variance\" is that if\n",
      "any of the input points are varied slightly, it could result in a very different model.\n",
      "\n",
      "In the middle, for `d = 2`, we have found a good mid-point. It fits\n",
      "the data fairly well, and does not suffer from the bias and variance\n",
      "problems seen in the figures on either side. What we would like is a\n",
      "way to quantitatively identify bias and variance, and optimize the\n",
      "metaparameters (in this case, the polynomial degree d) in order to\n",
      "determine the best algorithm. This can be done through a process\n",
      "called *validation*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Let's do an Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We'll create a simple nonlinear function that we'd like to fit\n",
      "def test_func(x, err=0.5):\n",
      "    y = 10 - 1. / (x + 0.1)\n",
      "    if err > 0:\n",
      "        y = np.random.normal(y, err)\n",
      "    return y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now let's create a realization of this dataset\n",
      "def make_data(N=40, error=1.0, random_seed=1):\n",
      "    # randomly sample the data\n",
      "    np.random.seed(1)\n",
      "    X = np.random.random(N)[:, np.newaxis]\n",
      "    y = test_func(X.ravel(), error)\n",
      "    \n",
      "    return X, y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X, y = make_data(40, error=1)\n",
      "X, y = make_data(100, error=1.0)\n",
      "plt.scatter(X.ravel(), y)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "model = LinearRegression()\n",
      "model.fit(X, y)\n",
      "y_test = model.predict(X_test)\n",
      "\n",
      "plt.scatter(X.ravel(), y)\n",
      "plt.plot(X_test.ravel(), y_test)\n",
      "print \"mean squared error:\", metrics.mean_squared_error(model.predict(X), y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "from sklearn.pipeline import make_pipeline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
      "\n",
      "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
      "model.fit(X, y)\n",
      "y_test = model.predict(X_test)\n",
      "\n",
      "plt.scatter(X.ravel(), y)\n",
      "plt.plot(X_test.ravel(), y_test)\n",
      "print \"mean squared error:\", metrics.mean_squared_error(model.predict(X), y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Detecting over-fitting\n",
      "Clearly, computing the error on the training data is not enough (we saw this previously).  But computing this *training error* can help us determine what's going on: in particular, comparing the training error and the validation error can give you an indication of how well your data is being fit.\n",
      "\n",
      "Let's do this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "degrees = np.arange(1, 30)\n",
      "\n",
      "X, y = make_data(100, error=1.0)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
      "\n",
      "training_error = []\n",
      "test_error = []\n",
      "mse = metrics.mean_squared_error\n",
      "\n",
      "for d in degrees:\n",
      "    model = make_pipeline(PolynomialFeatures(degree=d), LinearRegression())\n",
      "    model.fit(X_train, y_train)\n",
      "    training_error.append(mse(model.predict(X_train), y_train))\n",
      "    test_error.append(mse(model.predict(X_test), y_test))\n",
      "    \n",
      "# note that the test error can also be computed via cross-validation\n",
      "plt.plot(degrees, training_error, label='training')\n",
      "plt.plot(degrees, test_error, label='test')\n",
      "plt.legend()\n",
      "plt.xlabel('degree')\n",
      "plt.ylabel('MSE')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Another Example - When should you stop to Train"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_func(x, err=0.5):\n",
      "    return np.random.normal(10 - 1. / (x + 0.1), err)\n",
      "\n",
      "def compute_error(x, y, p):\n",
      "    yfit = np.polyval(p, x)\n",
      "    return np.sqrt(np.mean((y - yfit) ** 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 200\n",
      "test_size = 0.4\n",
      "error = 1.0\n",
      "\n",
      "# randomly sample the data\n",
      "np.random.seed(1)\n",
      "x = np.random.random(N)\n",
      "y = test_func(x, error)\n",
      "\n",
      "# split into training, validation, and testing sets.\n",
      "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=test_size)\n",
      "\n",
      "# show the training and validation sets\n",
      "plt.scatter(xtrain, ytrain, color='red')\n",
      "plt.scatter(xtest, ytest, color='blue')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# suppress warnings from Polyfit\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore', message='Polyfit*')\n",
      "\n",
      "degrees = np.arange(21)\n",
      "train_err = np.zeros(len(degrees))\n",
      "validation_err = np.zeros(len(degrees))\n",
      "\n",
      "for i, d in enumerate(degrees):\n",
      "    p = np.polyfit(xtrain, ytrain, d)\n",
      "\n",
      "    train_err[i] = compute_error(xtrain, ytrain, p)\n",
      "    validation_err[i] = compute_error(xtest, ytest, p)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "\n",
      "ax.plot(degrees, validation_err, lw=2, label = 'cross-validation error')\n",
      "ax.plot(degrees, train_err, lw=2, label = 'training error')\n",
      "ax.plot([0, 20], [error, error], '--k', label='intrinsic error')\n",
      "\n",
      "ax.legend(loc=0)\n",
      "ax.set_xlabel('degree of fit')\n",
      "ax.set_ylabel('rms error')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Learning Curves\n",
      "\n",
      "A learning curve is a plot of the training and validation\n",
      "error as a function of the number of training points. Note that\n",
      "when we train on a small subset of the training data, the training\n",
      "error is computed using this subset, not the full training set.\n",
      "These plots can give a quantitative view into how beneficial it\n",
      "will be to add training samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# suppress warnings from Polyfit\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore', message='Polyfit*')\n",
      "\n",
      "def plot_learning_curve(d, N=200):\n",
      "    n_sizes = 50\n",
      "    n_runs = 10\n",
      "    sizes = np.linspace(2, N, n_sizes).astype(int)\n",
      "    train_err = np.zeros((n_runs, n_sizes))\n",
      "    validation_err = np.zeros((n_runs, n_sizes))\n",
      "    for i in range(n_runs):\n",
      "        for j, size in enumerate(sizes):\n",
      "            xtrain, xtest, ytrain, ytest = train_test_split(\n",
      "                x, y, test_size=test_size, random_state=i)\n",
      "            # Train on only the first `size` points\n",
      "            p = np.polyfit(xtrain[:size], ytrain[:size], d)\n",
      "            \n",
      "            # Validation error is on the *entire* validation set\n",
      "            validation_err[i, j] = compute_error(xtest, ytest, p)\n",
      "            \n",
      "            # Training error is on only the points used for training\n",
      "            train_err[i, j] = compute_error(xtrain[:size], ytrain[:size], p)\n",
      "\n",
      "    fig, ax = plt.subplots()\n",
      "    ax.plot(sizes, validation_err.mean(axis=0), lw=2, label='mean validation error')\n",
      "    ax.plot(sizes, train_err.mean(axis=0), lw=2, label='mean training error')\n",
      "    ax.plot([0, N], [error, error], '--k', label='intrinsic error')\n",
      "\n",
      "    ax.set_xlabel('traning set size')\n",
      "    ax.set_ylabel('rms error')\n",
      "    \n",
      "    ax.legend(loc=0)\n",
      "    \n",
      "    ax.set_xlim(0, N-1)\n",
      "\n",
      "    ax.set_title('d = %i' % d)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've defined this function, we can plot the learning curve.\n",
      "But first, take a moment to think about what we're going to see:\n",
      "\n",
      "**Questions:**\n",
      "\n",
      "- **As the number of training samples are increased, what do you expect to see for the training error?  For the validation error?**\n",
      "- **Would you expect the training error to be higher or lower than the validation error?  Would you ever expect this to change?**\n",
      "\n",
      "We can run the following code to plot the learning curve for a ``d=1`` model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_learning_curve(d=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}